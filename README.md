# LangAlpha

 **Note**: Stocksflags is now renamed to LangAlpha
 
LangAlpha is a multi-agent AI equity analysis tool designed to provide comprehensive insights into the stock market. It leverages Large Language Models (LLMs) and agentic workflows to automate data gathering, processing, and analysis.

## Example Analysis

Here are some examples of stock analyses generated by LangAlpha:
- [NVIDIA Year-to-Date Analysis](/assets/nvidia_ytd.md)
- [NVIDIA Two-Month Analysis](/assets/nvidia_two_month.md)
- [Tarrif and Trade War Analysis](/assets/tarrif_and_trade_war.md)
- [Market Summary April 23](/assets/market_summary_april_23.md)
- [Nvidia Report](/assets/nvdia_report.md)
- [Tesla Report](/assets/tesla_report.md)
- [Chinese EV Company Report](/assets/chinese_ev_company_report.md)


Depends on the query, it take 2-6 minutes to generate the response. Token usage is generally around 30000+ to 100000+ tokens. Maxium token usage observed is 490000+ tokens.

Examples are generated in ealier phase of the development. Run the latest version for better result.


## Key Technologies

*   **Programming Language:** Python
*   **AI/LLM Frameworks:** LangChain, LangGraph
*   **Core Agent Workflow:** Implemented in `src/agent/market_intelligence_agent` using LangGraph
*   **Data Sources & Tools:**
    *   **Market/Fundamental Data:** Polygon, Yahoo Finance (via `tools/market_data.py`, `tools/fundamental_data.py`)
    *   **News/Web Research:** Tavily Search, Tickertick News API (via `tools/tavily.py`, `tools/tickertick.py`)
    *   **Web Browsing:** Playwright (integrated via `browser` agent)
    *   **Code Execution:** Local Python/Bash environment (via `coder` agent)
    *   **Database:** We use mongoDB to store the query history and the analysis result in production phase
*   **Environment Management:** `uv`

## Core Functionality: Market Intelligence Agent Workflow

The primary functionality is delivered through a sophisticated **multi-agent system** built with **LangGraph**, located in `src/agent/market_intelligence_agent/`.

**Workflow Overview:**

1.  **User Query:** The system accepts natural language queries about financial markets, stocks, or economic events.
2.  **Supervisor & Planner:** A `supervisor` agent oversees the entire process. It first consults a `planner` agent, which breaks down the user's request into a detailed, step-by-step research and analysis plan.
3.  **Information Gathering (Parallel & Iterative):**
    *   The `supervisor` delegates tasks based on the plan to specialized agents:
        *   **`researcher`:** Fetches news (Tickertick) and performs general web searches (Tavily) for qualitative information, recent events, and context.
        *   **`market`:** Retrieves quantitative data like stock prices, technical indicators, fundamental data (financials, valuation), and related metrics using dedicated tools connected to Polygon and Yahoo Finance.
        *   **`browser`:** (Used sparingly due to cost) Performs deep web browsing for specific, hard-to-find information on given URLs when the `researcher`'s tools are insufficient.
4.  **Analysis & Synthesis:**
    *   **`coder`:** (If required by the plan) Executes Python code for complex calculations, data manipulation, or analysis beyond the `market` agent's built-in capabilities.
    *   **`analyst`:** Acts as a financial expert (L/S Hedge Fund perspective). It synthesizes the information gathered by all other agents, identifies key insights, assesses risks, generates potential investment theses (Long/Short), and provides actionable financial analysis based *only* on the provided data.
5.  **Reporting:**
    *   The `supervisor` reviews the findings from all agents, potentially iterating or requesting clarifications.
    *   Finally, a `reporter` agent compiles all the validated information and analysis into a comprehensive, well-structured Markdown report, including tables and summaries.

**Key Capabilities:**

*   **Autonomous Research:** Agents collaboratively gather diverse data types (news, market data, fundamentals, web content).
*   **In-depth Analysis:** Combines quantitative data retrieval with qualitative research and expert financial analysis.
*   **Structured Planning:** Ensures a logical flow of information gathering and analysis tailored to the user's query.
*   **Flexible Orchestration:** The `supervisor` dynamically routes tasks and manages the workflow, allowing for iteration and refinement.
*   **Actionable Insights:** Aims to provide not just data, but synthesized analysis suitable for investment decision-making context (though not direct financial advice).

Below is an image demonstrate the current agent workflow
![graph](/assets/graph.png)

## Damodaran Valuation Model:
- Implements valuation methodologies inspired by Professor Aswath Damodaran.
- Provides a user interface (details TBD) for manual input and valuation generation.
- See [notebooks/ginzu_interface.ipynb](notebooks/ginzu_interface.ipynb) for details.

## Trading Strategy:
- See [counter-trend-trading-strategy colab](https://colab.research.google.com/drive/1Wo1f5SvZ3M9YjUx7gl4q2rSIo7PMegBN?usp=sharing#scrollTo=1kA-HcwGkK9Z) for details.

## Repository Structure
```
LangAlpha/
├── data                                  # Data directory
├── models                                # Valuation Model directory
├── notebooks/                            # Jupyter notebooks for demonstration
|    ├── checkpoint/
|    |    ├── milestone_3.ipynb             # Checkpoint for milestone 3
|    |    └── milestone_4.ipynb             # Checkpoint for milestone 4
|    ├── demo/                              # Notebook for demo and testing                         
|    └── db_management.ipynb                # Database management tool                 
├── src/                                    # Source code
|    ├── agent/                           # Agent directory
|    |    └── market_intelligence_agent/    # Market Intelligence Agent
|    |         ├── agents/                  # Agent implementations
|    |         ├── prompts/                 # Agent prompts
|    |         ├── tools/                   # Agent tools
|    |         ├── config/                  # Configuration files
|    |         ├── graph/                   # Agent workflow graphs
|    |         ├── service/                 # Service implementations
|    |         ├── crawler/                 # Web crawlers
|    |         └── __init__.py              # Package initialization
|    ├── database/                         # Database setup and utilities
|    |    ├── models/                       # Database models/schemas
|    |    ├── utils/                        # Database utility functions
|    |    └── Dockerfile                    # Dockerfile for database service
|    ├── web/                              # Frontend Web Application (FastAPI)
|    |    ├── static/                       # Static assets (CSS, JS)
|    |    ├── templates/                    # HTML templates
|    |    ├── main.py                       # Main FastAPI application
|    |    └── Dockerfile                    # Dockerfile for web service
|    ├── data_tool/                      # Tools for data retriving
|    |    ├── data_providers/             
|    |    |    ├── connect_wrds.py          # code to connect wrds
|    |    |    ├── financial_datasets.py    # code to retrieve data from financial datasets
|    |    |    ├── polygon.py               # code to retrieve data from polygon
|    |    |    └── yahoo_finance.py         # code to retrieve data from yahoo finance
|    |    ├── data_models.py                # pydantic models
|    |    └── get_data.py                   # get data
|    ├── database_tool/                  # Tools for Database Manipulation   
|    |    ├── connect_db.py                 # connect to database
|    |    ├── create_table.py               # create tables
|    |    └── db_operation.py               # complax data retrieval from database
|    └── llm/                            # LLM config for LLM use oustide of Langraph workflow
|         ├── llm_models.py                 # LLM models
|         └── api_call.py                   # Make api call to LLM
|  
└── ...
```

## Getting Started

### 1. Clone the Repository
```bash
# Clone the repository to your local machine
git clone https://github.com/Chen-zexi/LangAlpha.git

# Navigate to the project directory
cd LangAlpha
```

### 2. Docker Setup with Web UI (Recommended)

This project is configured to run using Docker Compose, which simplifies the setup of the application, database, and other langraph services.

1.  **Install Docker and Docker Compose:** Ensure you have Docker Desktop (or Docker Engine + Docker Compose) installed on your system. You can download it from the [official Docker website](https://www.docker.com/products/docker-desktop/).

2.  **Configure Environment Variables:**
    *   Copy the example environment file:
        ```bash
        cp .env.example .env
        ```
    *   Edit the `.env` file and replace the placeholder values (`replace_with_your...`) with your actual API keys and database credentials. **Crucially**, ensure the `MONGODB_URI` is set correctly for Docker networking (e.g., `mongodb://admin:password@mongodb:27017/`). The default value in `.env.example` should work if you don't change the service name or credentials in `docker-compose.yml`.

3.  **Build and Run with Docker Compose:**
    *   Navigate to the project's root directory (where `docker-compose.yml` is located).
    *   Run the following command:
        ```bash
        docker-compose up --build -d
        ```
        *   `--build`: Forces Docker Compose to rebuild the images if the Dockerfiles or related source code have changed.
        *   `-d`: Runs the containers in detached mode (in the background).
    *   This command will:
        *   Build the Docker images for the `web`, `database`, and other langraph services specified in `docker-compose.yml`.
        *   Start the containers for all services.
        *   Set up the necessary network connections between the services.

4.  **Access the Application:** Once the containers are running, you should be able to access the web application by navigating to `http://localhost:8000` (or the port specified in `docker-compose.yml` and the web service configuration) in your web browser.
 
    **Note:** Browser is not supported in Web UI. Please use the local version instead.

5.  **Stopping the Application:**
    *   To stop the running containers, navigate to the project root and run:
        ```bash
        docker-compose down
        ```

### Manual Setup without Web UI and Docker

#### 1. Environment Setup

It is recommended to use `uv` for managing Python virtual environments and packages.

```bash
# Install uv (if you haven't already)
pip install uv

# Create a virtual environment named .venv in the project root
uv venv

# Activate the virtual environment
# On macOS/Linux:
source .venv/bin/activate

# On Windows:
 .venv\\Scripts\\activate

# Install dependencies from requirements.txt
uv pip sync requirements.lock
# or
uv pip install -e .
# or
uv pip install -r requirements.txt

# Install playwright for browser agent
uv run playwright install
```

You can also install the dependencies manually.

```bash
pip install -r requirements.txt
```


#### 2. Run the Application

1.  **Set up API Keys:**
    *   Create a `.env` file if you haven't already:
        ```bash
        cp .env.example .env
        ```
    *   Edit the `.env` file and fill in your required API keys and credentials.

2.  **Choose How to Run:**

    *   **Option A: Run with Web UI (FastAPI):**
        ```bash
        uv run src/web/run_server.py
        ```
        Then access the application in your browser (usually `http://localhost:8000`).

    *   **Option B: Run CML Version (Agent Workflow Only):**
        *   **Standard CML:**
            ```bash
            uv run main.py 
            ``` 
            _(Note: Adjust path if needed, e.g., `uv run src/main.py` depending on your entry point)_
            The CML version will typically prompt you for input in the terminal. Results are often saved to `assets/reports`.

        *   **CML with LangGraph Studio (for Debugging/Visualization):**
            ```bash
            # Install langgraph-cli if you haven't
            uv pip install langgraph-cli

            # Navigate to the agent directory
            cd src/agent 

            # Start the LangGraph Studio server
            langgraph dev & # Run in background

            # Navigate back to the project root
            # cd ../.. 

            # Run the main CML script in development mode, connecting to the studio
            uv run main.py -dev 
            ```
            Access the LangGraph Studio interface in your browser (usually `http://localhost:1984`) to visualize the agent workflow.

## Contributors and contributions: 
**Alan** zc2610@nyu.edu
- Langraph pipeline (Market Intelligence Workflow)
- Data tool integration
- Repo management
- Frontend interface and web development

**Tyler** tan4742@nyu.edu
- Ginzu interface (valuation model from Professor Aswath Damodaran) 
- Time series analysis

**Jackson** jc13246@nyu.edu
- Report Writing and Frontend Interface

**April** asl8466@nyu.edu
- Frontend Interface and Trading Strategy

**Vinci** cc9100@nyu.edu
- Trading Strategy

## Acknowledgements
This project is inspired by the following projects:

**[Langchain](https://github.com/langchain-ai/langchain)**

**[Langraph](https://github.com/langchain-ai/langgraph)**

**[Langchain MCP Adapter](https://github.com/langchain-ai/langchain-mcp-adapters)**

**[Langmanus](https://github.com/Darwin-lfl/langmanus)**

**[Ai-Hedge-Fund](https://github.com/virattt/ai-hedge-fund)**

**[Browser-Use](https://github.com/browser-use/browser-use)**

**[TickerTick-API](https://github.com/hczhu/TickerTick-API)**

## Citation
```bibtex
@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
```