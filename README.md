# LangAlpha

 **Note**: Stocksflags is now renamed to LangAlpha
 
LangAlpha is a multi-agent AI equity analysis tool designed to provide comprehensive insights into the stock market. It leverages Large Language Models (LLMs) and agentic workflows to automate data gathering, processing, and analysis.

## Example Analysis

Here are some examples of stock analyses generated by LangAlpha:
- [NVIDIA Year-to-Date Analysis](/assets/nvidia_ytd.md)
- [NVIDIA Two-Month Analysis](/assets/nvidia_two_month.md)
- [Tarrif and Trade War Analysis](/assets/tarrif_and_trade_war.md)
- [Market Summary April 23](/assets/market_summary_april_23.md)
- [Nvidia Report](/assets/nvdia_report.md)
- [Tesla Report](/assets/tesla_report.md)
- [Chinese EV Company Report](/assets/chinese_ev_company_report.md)


Depends on the query, it take 2-6 minutes to generate the response. Token usage is generally around 30000+ to 100000+ tokens. Maxium token usage observed is 490000+ tokens.

Examples are generated in different phase of the development. Result may vary.


## Key Technologies

*   **Programming Language:** Python
*   **AI/LLM Frameworks:** LangChain, LangGraph
*   **Core Agent Workflow:** Implemented in `src/agent/market_intelligence_agent` using LangGraph
*   **Data Sources & Tools:**
    *   **Market/Fundamental Data:** Polygon, Yahoo Finance (via `src/mcp_server/market_data.py`, `src/mcp_server/fundamental_data.py`)
    *   **News/Web Research:** Tavily Search, Tickertick News API (via `src/mcp_server/tavily.py`, `src/mcp_server/tickertick.py`)
    *   **Web Browsing:** Playwright (integrated via `browser` agent)
    *   **Code Execution:** Local Python/Bash environment (via `coder` agent)
    *   **Database:** We will use mongoDB to store the query history and the analysis result in production phase
*   **Environment Management:** `uv`

## Core Functionality: Market Intelligence Agent Workflow

The primary functionality is delivered through a sophisticated **multi-agent system** built with **LangGraph**, located in `src/agent/market_intelligence_agent/`.

**Workflow Overview:**

1.  **User Query:** The system accepts natural language queries about financial markets, stocks, or economic events.
2.  **Supervisor & Planner:** A `supervisor` agent oversees the entire process. It first consults a `planner` agent, which breaks down the user's request into a detailed, step-by-step research and analysis plan.
3.  **Information Gathering (Parallel & Iterative):**
    *   The `supervisor` delegates tasks based on the plan to specialized agents:
        *   **`researcher`:** Fetches news (Tickertick) and performs general web searches (Tavily) for qualitative information, recent events, and context.
        *   **`market`:** Retrieves quantitative data like stock prices, technical indicators, fundamental data (financials, valuation), and related metrics using dedicated tools connected to Polygon and Yahoo Finance.
        *   **`browser`:** (Used sparingly due to cost) Performs deep web browsing for specific, hard-to-find information on given URLs when the `researcher`'s tools are insufficient.
4.  **Analysis & Synthesis:**
    *   **`coder`:** (If required by the plan) Executes Python code for complex calculations, data manipulation, or analysis beyond the `market` agent's built-in capabilities.
    *   **`analyst`:** Acts as a financial expert (L/S Hedge Fund perspective). It synthesizes the information gathered by all other agents, identifies key insights, assesses risks, generates potential investment theses (Long/Short), and provides actionable financial analysis based *only* on the provided data.
5.  **Reporting:**
    *   The `supervisor` reviews the findings from all agents, potentially iterating or requesting clarifications.
    *   Finally, a `reporter` agent compiles all the validated information and analysis into a comprehensive, well-structured Markdown report, including tables and summaries.

**Key Capabilities:**

*   **Autonomous Research:** Agents collaboratively gather diverse data types (news, market data, fundamentals, web content).
*   **In-depth Analysis:** Combines quantitative data retrieval with qualitative research and expert financial analysis.
*   **Structured Planning:** Ensures a logical flow of information gathering and analysis tailored to the user's query.
*   **Flexible Orchestration:** The `supervisor` dynamically routes tasks and manages the workflow, allowing for iteration and refinement.
*   **Actionable Insights:** Aims to provide not just data, but synthesized analysis suitable for investment decision-making context (though not direct financial advice).

Below is an image demonstrate the current agent workflow
![graph](/assets/graph.png)

## Damodaran Valuation Model:
- Implements valuation methodologies inspired by Professor Aswath Damodaran.
- Provides a user interface (details TBD) for manual input and valuation generation.
- See [notebooks/ginzu_interface.ipynb](notebooks/ginzu_interface.ipynb) for details.

## Trading Strategy:
- See [counter-trend-trading-strategy colab](https://colab.research.google.com/drive/1Wo1f5SvZ3M9YjUx7gl4q2rSIo7PMegBN?usp=sharing#scrollTo=1kA-HcwGkK9Z) for details.

## MongoDB Integration

The application uses MongoDB to store streaming messages and generated reports. This allows for:

1. History tracking: Users can view their past queries and the conversations.
2. Report storage: All generated reports are stored for later reference.

### MongoDB Structure

- **Collections**:
  - `messages`: Stores all stream messages including user queries, assistant responses, and system notifications.
  - `reports`: Stores the final generated reports.

### Environment Variables

MongoDB configuration is controlled via the following environment variables:

```
MONGODB_URI=mongodb://admin:password@mongodb:27017/
MONGODB_DB=langalpha
```

### Accessing History

The application provides the following endpoints for accessing historical data:

- `/history`: A UI page that lists all previous sessions.
- `/history/{session_id}`: A UI page that shows a specific session with its messages and reports.
- `/api/history/sessions`: API endpoint that returns a list of all sessions.
- `/api/history/messages/{session_id}`: API endpoint that returns all messages for a specific session.
- `/api/history/reports/{session_id}`: API endpoint that returns all reports for a specific session.
- `/api/history/report/{report_id}`: API endpoint that returns a specific report by ID.

## Repository Structure
```
LangAlpha/
├── data                                  # Data directory
├── models                                # Valuation Model directory
├── notebooks/                            # Jupyter notebooks for demonstration
|    ├── checkpoint/
|    |    ├── milestone_3.ipynb             # Checkpoint for milestone 3
|    |    └── milestone_4.ipynb             # Checkpoint for milestone 4
|    ├── demo/                              # Notebook for demo and testing                         
|    └── db_management.ipynb                # Database management tool                 
├── src/                                    # Source code
|    ├── agent/                           # Agent directory
|    |    └── market_intelligence_agent/    # Market Intelligence Agent
|    |         ├── agents/                  # Agent implementations
|    |         ├── prompts/                 # Agent prompts
|    |         ├── tools/                   # Agent tools
|    |         ├── config/                  # Configuration files
|    |         ├── graph/                   # Agent workflow graphs
|    |         ├── service/                 # Service implementations
|    |         ├── crawler/                 # Web crawlers
|    |         └── __init__.py              # Package initialization
|    ├── data_tool/                      # Tools for data retriving
|    |    ├── data_providers/             
|    |    |    ├── connect_wrds.py          # code to connect wrds
|    |    |    ├── financial_datasets.py    # code to retrieve data from financial datasets
|    |    |    ├── polygon.py               # code to retrieve data from polygon
|    |    |    └── yahoo_finance.py         # code to retrieve data from yahoo finance
|    |    ├── data_models.py                # pydantic models
|    |    └── get_data.py                   # get data
|    ├── database_tool/                  # Tools for Database Manipulation   
|    |    ├── connect_db.py                 # connect to database
|    |    ├── create_table.py               # create tables
|    |    └── db_operation.py               # complax data retrieval from database
|    └── llm/                            # LLM config for LLM use oustide of Langraph workflow
|         ├── llm_models.py                 # LLM models
|         └── api_call.py                   # Make api call to LLM
|  
└── ...
```

## Getting Started

### 1. Clone the Repository
```bash
# Clone the repository to your local machine
git clone https://github.com/Chen-zexi/LangAlpha.git

# Navigate to the project directory
cd LangAlpha
```

### 2. Environment Setup

It is recommended to use `uv` for managing Python virtual environments and packages.

```bash
# Install uv (if you haven't already)
pip install uv

# Create a virtual environment named .venv in the project root
uv venv

# Activate the virtual environment
# On macOS/Linux:
source .venv/bin/activate

# On Windows:
 .venv\\Scripts\\activate

# Install dependencies from requirements.txt
uv pip sync requirements.lock
# or
uv pip install -e .
# or
uv pip install -r requirements.txt

# Install playwright for browser agent
uv run playwright install
```

You can also install the dependencies manually.

```bash
pip install -r requirements.txt
```


### 3. Set up API Keys
1. Create a `.env` file.

```bash
cp .env.example .env
```

2. Define required API keys in your `.env` file.

### 4. Run the project

1. Run the project with uv
```bash
uv run main.py
```

2. Run the project with langraph studio

```bash
# Install langgraph-cli
pip install langgraph-cli

# Navigate to the src/agent directory
cd src/agent

# Run the langgraph studio
langgraph dev --allow-blocking

# Run the main.py with dev mode
uv run main.py -dev
```
Response will be saved in `assets/reports` folder in markdown format.
## Contributors and contributions: 
**Alan** zc2610@nyu.edu
- Langraph pipeline (Market Intelligence Workflow)
- Data tool integration
- Repo management

**Tyler** tan4742@nyu.edu
- Ginzu interface (valuation model from Professor Aswath Damodaran) 
- Time series analysis

**Jackson** jc13246@nyu.edu
- Report Writing and Frontend Interface

**April** asl8466@nyu.edu
- Frontend Interface and Trading Strategy

**Vinci** cc9100@nyu.edu
- Trading Strategy

## Acknowledgements
This project is inspired by the following projects:

**[Langchain](https://github.com/langchain-ai/langchain)**

**[Langraph](https://github.com/langchain-ai/langgraph)**

**[Langchain MCP Adapter](https://github.com/langchain-ai/langchain-mcp-adapters)**

**[Langmanus](https://github.com/Darwin-lfl/langmanus)**

**[Ai-Hedge-Fund](https://github.com/virattt/ai-hedge-fund)**

**[Browser-Use](https://github.com/browser-use/browser-use)**

**[TickerTick-API](https://github.com/hczhu/TickerTick-API)**

## Citation
```bibtex
@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
```