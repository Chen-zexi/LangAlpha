{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA', 'META', 'NFLX', 'NVDA', 'AMD', \n",
    "           'BA', 'V', 'SPY', 'SPCE', 'BABA', 'MSTR', 'DIS', 'PYPL', 'SHOP', 'COIN', \n",
    "            'INTC', 'CSCO', 'IBM', 'GE', 'WMT', 'T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = Downloader(\"NYU\", \"tan4742@nyu.edu\", \"../data/sec_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_financials(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        \n",
    "        # Extract relevant metadata information from the document\n",
    "        filing_data = {}\n",
    "\n",
    "        # Helper function to safely find the next tag for metadata\n",
    "        def safe_find(text):\n",
    "            tag = soup.find(text=text)\n",
    "            if tag:\n",
    "                return tag.find_next().text.strip() if tag.find_next() else None\n",
    "            return None\n",
    "        \n",
    "        # Extract metadata\n",
    "        filing_data[\"accession_number\"] = safe_find(\"ACCESSION NUMBER:\")\n",
    "        filing_data[\"conformed_submission_type\"] = safe_find(\"CONFORMED SUBMISSION TYPE:\")\n",
    "        filing_data[\"company_name\"] = safe_find(\"COMPANY CONFORMED NAME:\")\n",
    "        filing_data[\"fiscal_year_end\"] = safe_find(\"FISCAL YEAR END:\")\n",
    "        filing_data[\"date_of_filing\"] = safe_find(\"FILED AS OF DATE:\")\n",
    "        \n",
    "        # Extracting document metadata (if available)\n",
    "        filing_data[\"form_type\"] = safe_find(\"FORM TYPE:\")\n",
    "        filing_data[\"sec_act\"] = safe_find(\"SEC ACT:\")\n",
    "        filing_data[\"sec_file_number\"] = safe_find(\"SEC FILE NUMBER:\")\n",
    "        filing_data[\"film_number\"] = safe_find(\"FILM NUMBER:\")\n",
    "        \n",
    "        # Extracting business and mailing address (if available)\n",
    "        filing_data[\"business_address\"] = safe_find(\"BUSINESS ADDRESS:\")\n",
    "        filing_data[\"mail_address\"] = safe_find(\"MAIL ADDRESS:\")\n",
    "        \n",
    "        # Extracting HTML content from <DOCUMENT> for financials\n",
    "        document_tag = soup.find(\"DOCUMENT\")\n",
    "        if document_tag:\n",
    "            filing_data[\"document_html\"] = document_tag.text.strip()\n",
    "        \n",
    "        return filing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tickers(tickers, base_path):\n",
    "    all_data = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Set the base directory for this ticker\n",
    "        ticker_dir = os.path.join(base_path, ticker, \"8-K\")\n",
    "        \n",
    "        # Iterate through each submission folder for the ticker\n",
    "        for subdir, _, files in os.walk(ticker_dir):\n",
    "            for file in files:\n",
    "                if file == \"full-submission.txt\":\n",
    "                    file_path = os.path.join(subdir, file)\n",
    "                    \n",
    "                    # Extract the relevant data from the full submission file\n",
    "                    filing_data = extract_financials(file_path)\n",
    "                    \n",
    "                    # Add extracted data to the list\n",
    "                    all_data.append(filing_data)\n",
    "    \n",
    "    # Convert the list of dictionaries into a pandas DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV or JSON file\n",
    "    df.to_csv(\"data/cleaned_tables/filing_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data/sec_data/sec-edgar-filings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zc/mk397vzd3rndczvtnq2xwwzr0000gn/T/ipykernel_3426/4192151999.py:10: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tag = soup.find(text=text)\n"
     ]
    },
    {
     "ename": "ParserRejectedMarkup",
     "evalue": "The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n\nOriginal exception(s) from parser:\n AssertionError: expected name token at '<![(V#1(2\\\\Y32.RXI/Y('",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserRejectedMarkup\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_tickers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 15\u001b[0m, in \u001b[0;36mprocess_tickers\u001b[0;34m(tickers, base_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subdir, file)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract the relevant data from the full submission file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m filing_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_financials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Add extracted data to the list\u001b[39;00m\n\u001b[1;32m     18\u001b[0m all_data\u001b[38;5;241m.\u001b[39mappend(filing_data)\n",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m, in \u001b[0;36mextract_financials\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_financials\u001b[39m(file_path):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m         soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Extract relevant metadata information from the document\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         filing_data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:482\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m    481\u001b[0m     other_exceptions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m rejections]\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserRejectedMarkup(\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal exception(s) from parser:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(other_exceptions)\n\u001b[1;32m    485\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Clear out the markup and remove the builder's circular\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# reference to this object.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mParserRejectedMarkup\u001b[0m: The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n\nOriginal exception(s) from parser:\n AssertionError: expected name token at '<![(V#1(2\\\\Y32.RXI/Y('"
     ]
    }
   ],
   "source": [
    "process_tickers(tickers, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
