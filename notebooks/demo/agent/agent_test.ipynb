{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"GEMINI_API_KEY\")\n",
    "#_set_if_undefined(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "#from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=os.getenv('GEMINI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../../..'))\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "members = [\"news_analyst\", \"coder\"]\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    f\" following workers: {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "\n",
    "def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744396976.599783 16200342 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "async def make_news_agent():     \n",
    "    async with MultiServerMCPClient(\n",
    "        {\n",
    "            \"tickertick\": {\n",
    "                \"command\": \"python\",\n",
    "                # Make sure to update to the full absolute path to your math_server.py file\n",
    "                \"args\": [\"/Users/chen/Library/Mobile Documents/com~apple~CloudDocs/NYU/SPRING 25/TECH-UB 24/StocksFlags/src/mcp_server/tickertick.py\"],\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "        }\n",
    "    ) as client:\n",
    "        tickertick = client.get_tools()\n",
    "        agent = create_react_agent(llm, tools=tickertick, prompt=\"You are a news analyst. You get the news based on user's request and return the summary of the news.\")\n",
    "    return agent\n",
    "\n",
    "news_analyst_agent = await make_news_agent()\n",
    "\n",
    "async def news_analyst_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = await news_analyst_agent.ainvoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"news_analyst\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
    "\n",
    "\n",
    "def code_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = code_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"news_analyst\", news_analyst_node)\n",
    "builder.add_node(\"coder\", code_node)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744397191.187657 16200342 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "news_analyst_agent = await make_news_agent()\n",
    "query = \"What's some news about nvda?\"\n",
    "response = await news_analyst_agent.ainvoke({\"messages\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's some news about nvda?\", additional_kwargs={}, response_metadata={}, id='0a0ef151-bd86-49a1-bda9-a93a375e05c9'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_ticker_news_tool', 'arguments': '{\"ticker\": \"NVDA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-36b7ada9-3b38-4742-8e2e-44d146fb1c5f-0', tool_calls=[{'name': 'get_ticker_news_tool', 'args': {'ticker': 'NVDA'}, 'id': 'a4da0c54-d890-45ac-bd94-c82af643997c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 661, 'output_tokens': 10, 'total_tokens': 671, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Error: ClosedResourceError()\\n Please fix your mistakes.', name='get_ticker_news_tool', id='0e21d798-3326-4ad9-a2c5-ee52526bcb81', tool_call_id='a4da0c54-d890-45ac-bd94-c82af643997c', status='error'), AIMessage(content='I am sorry, I cannot fulfill this request. There was an error when retrieving the information. Please try again.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-03229a2e-98f2-4945-b853-1c7b18c4a8e6-0', usage_metadata={'input_tokens': 691, 'output_tokens': 24, 'total_tokens': 715, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chen/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1410: UserWarning: ChatGoogleGenerativeAI.with_structured_output with dict schema has changed recently to align with behavior of other LangChain chat models. More context: https://github.com/langchain-ai/langchain-google/pull/772\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'supervisor': {'next': 'news_analyst'}})\n",
      "----\n",
      "(('news_analyst:b9e553c5-8548-c12c-10ab-e34a2240d102',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_ticker_news_tool', 'arguments': '{\"ticker\": \"NVDA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-9309ae9d-63b2-4d9a-924d-471bd9e1f269-0', tool_calls=[{'name': 'get_ticker_news_tool', 'args': {'ticker': 'NVDA'}, 'id': 'ff28a9d1-41e3-49d0-8528-58af4b15d9f1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 661, 'output_tokens': 10, 'total_tokens': 671, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "(('news_analyst:b9e553c5-8548-c12c-10ab-e34a2240d102',), {'tools': {'messages': [ToolMessage(content='Error: ClosedResourceError()\\n Please fix your mistakes.', name='get_ticker_news_tool', id='5c865661-2109-4fc1-b9ec-89fdd21a4155', tool_call_id='ff28a9d1-41e3-49d0-8528-58af4b15d9f1', status='error')]}})\n",
      "----\n",
      "(('news_analyst:b9e553c5-8548-c12c-10ab-e34a2240d102',), {'agent': {'messages': [AIMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-50ca9390-05a7-4cd8-abea-4bed31f3cc03-0', usage_metadata={'input_tokens': 691, 'output_tokens': 24, 'total_tokens': 715, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "((), {'news_analyst': {'messages': [HumanMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={}, name='news_analyst')]}})\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chen/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1410: UserWarning: ChatGoogleGenerativeAI.with_structured_output with dict schema has changed recently to align with behavior of other LangChain chat models. More context: https://github.com/langchain-ai/langchain-google/pull/772\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'supervisor': {'next': 'news_analyst'}})\n",
      "----\n",
      "(('news_analyst:673a2636-61ad-6611-9ca9-9e6b0dcf13d5',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_ticker_news_tool', 'arguments': '{\"ticker\": \"NVDA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-8208d490-3221-421b-bb5d-6044da10fd12-0', tool_calls=[{'name': 'get_ticker_news_tool', 'args': {'ticker': 'NVDA'}, 'id': '95cf4f30-5bd2-42b1-a0a5-e09f03131747', 'type': 'tool_call'}], usage_metadata={'input_tokens': 685, 'output_tokens': 10, 'total_tokens': 695, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "(('news_analyst:673a2636-61ad-6611-9ca9-9e6b0dcf13d5',), {'tools': {'messages': [ToolMessage(content='Error: ClosedResourceError()\\n Please fix your mistakes.', name='get_ticker_news_tool', id='3523c6ce-f927-430f-bf19-12cea2e3a6b7', tool_call_id='95cf4f30-5bd2-42b1-a0a5-e09f03131747', status='error')]}})\n",
      "----\n",
      "(('news_analyst:673a2636-61ad-6611-9ca9-9e6b0dcf13d5',), {'agent': {'messages': [AIMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-895c93a8-9a85-49a6-aad8-cbb73a22d133-0', usage_metadata={'input_tokens': 715, 'output_tokens': 24, 'total_tokens': 739, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "((), {'news_analyst': {'messages': [HumanMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={}, name='news_analyst')]}})\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chen/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1410: UserWarning: ChatGoogleGenerativeAI.with_structured_output with dict schema has changed recently to align with behavior of other LangChain chat models. More context: https://github.com/langchain-ai/langchain-google/pull/772\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'supervisor': {'next': 'news_analyst'}})\n",
      "----\n",
      "(('news_analyst:485c062d-0a6c-4770-0121-7168e49a88cb',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_ticker_news_tool', 'arguments': '{\"ticker\": \"NVDA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-6d248c2a-2b70-40b8-a5e8-736f8ad2a229-0', tool_calls=[{'name': 'get_ticker_news_tool', 'args': {'ticker': 'NVDA'}, 'id': 'baa985de-64b1-41b3-953d-30a622b75a6b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 709, 'output_tokens': 10, 'total_tokens': 719, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "(('news_analyst:485c062d-0a6c-4770-0121-7168e49a88cb',), {'tools': {'messages': [ToolMessage(content='Error: ClosedResourceError()\\n Please fix your mistakes.', name='get_ticker_news_tool', id='edf17ffb-414b-494d-bff8-e676f2f4f5ce', tool_call_id='baa985de-64b1-41b3-953d-30a622b75a6b', status='error')]}})\n",
      "----\n",
      "(('news_analyst:485c062d-0a6c-4770-0121-7168e49a88cb',), {'agent': {'messages': [AIMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-b1cdeaba-b22e-4e5f-bacf-b708e063986a-0', usage_metadata={'input_tokens': 739, 'output_tokens': 24, 'total_tokens': 763, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "((), {'news_analyst': {'messages': [HumanMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={}, name='news_analyst')]}})\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chen/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1410: UserWarning: ChatGoogleGenerativeAI.with_structured_output with dict schema has changed recently to align with behavior of other LangChain chat models. More context: https://github.com/langchain-ai/langchain-google/pull/772\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'supervisor': {'next': 'news_analyst'}})\n",
      "----\n",
      "(('news_analyst:98114e51-28a0-31e1-c17e-2d67179d0165',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_ticker_news_tool', 'arguments': '{\"ticker\": \"NVDA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-08e13b89-96f2-455b-b3cd-dc6ed3dfc280-0', tool_calls=[{'name': 'get_ticker_news_tool', 'args': {'ticker': 'NVDA'}, 'id': '3936485c-afda-476c-8920-b7dcb53867f5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 733, 'output_tokens': 10, 'total_tokens': 743, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "(('news_analyst:98114e51-28a0-31e1-c17e-2d67179d0165',), {'tools': {'messages': [ToolMessage(content='Error: ClosedResourceError()\\n Please fix your mistakes.', name='get_ticker_news_tool', id='42551c70-5882-4aec-8337-3b5e9d274560', tool_call_id='3936485c-afda-476c-8920-b7dcb53867f5', status='error')]}})\n",
      "----\n",
      "(('news_analyst:98114e51-28a0-31e1-c17e-2d67179d0165',), {'agent': {'messages': [AIMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-176e6544-a308-4f6c-83e7-20d31732b582-0', usage_metadata={'input_tokens': 763, 'output_tokens': 24, 'total_tokens': 787, 'input_token_details': {'cache_read': 0}})]}})\n",
      "----\n",
      "((), {'news_analyst': {'messages': [HumanMessage(content='I am sorry, I cannot fulfill this request. There was an error reaching the API. Please try again in sometime.', additional_kwargs={}, response_metadata={}, name='news_analyst')]}})\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chen/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1410: UserWarning: ChatGoogleGenerativeAI.with_structured_output with dict schema has changed recently to align with behavior of other LangChain chat models. More context: https://github.com/langchain-ai/langchain-google/pull/772\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'supervisor': {'next': 'news_analyst'}})\n",
      "----\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms some news about nvda?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2651\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2651\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2652\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2653\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2654\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2655\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2656\u001b[0m     ):\n\u001b[1;32m   2657\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   2659\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/anaconda3/envs/stocksflags/lib/python3.11/site-packages/langgraph/pregel/runner.py:364\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    362\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m loop\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 364\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    365\u001b[0m         futures,\n\u001b[1;32m    366\u001b[0m         return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m    367\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, end_time \u001b[38;5;241m-\u001b[39m loop\u001b[38;5;241m.\u001b[39mtime()) \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stocksflags/lib/python3.11/asyncio/tasks.py:428\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m~/anaconda3/envs/stocksflags/lib/python3.11/asyncio/tasks.py:535\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    532\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "async for s in graph.astream(\n",
    "    {\"messages\": [(\"user\", \"What's some news about nvda?\")]}, subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Find the latest GDP of New York and California, then calculate the average\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocksflags",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
