{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerni/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base_transcripts_url = \"https://seekingalpha.com/symbol/{ticker}/earnings/transcripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "robots_url = \"https://seekingalpha.com/robots.txt\"\n",
    "response = requests.get(robots_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-agent: *\n",
      "Disallow: /api/v3/account\n",
      "Disallow: /articles/enqueue_tracking\n",
      "Disallow: /authentication\n",
      "Disallow: /clean\n",
      "Disallow: /iphone_data/check_update\n",
      "Disallow: /market_news/enqueue_tracking\n",
      "Disallow: /mone$\n",
      "Disallow: /mone_event\n",
      "Disallow: /mone_v2\n",
      "Disallow: /mpw_count\n",
      "Disallow: /research/enqueue_tracking\n",
      "Disallow: /_sa_track/\n",
      "Disallow: /xgCxM9By/init.js\n",
      "Disallow: /zuora\n",
      "\n",
      "Sitemap: https://seekingalpha.com/sitemap_news.xml\n",
      "Sitemap: https://seekingalpha.com/instablog/index.xml\n",
      "Sitemap: https://seekingalpha.com/news/index.xml\n",
      "Sitemap: https://seekingalpha.com/article/index.xml\n",
      "Sitemap: https://seekingalpha.com/author/index.xml\n",
      "Sitemap: https://seekingalpha.com/checkout/index.xml\n",
      "Sitemap: https://seekingalpha.com/symbol/sitemap_index.xml\n",
      "Sitemap: https://seekingalpha.com/evergreen_sitemap.xml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Failed to fetch robots.txt: Status {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_company_map = {\n",
    "    \"AAPL\": \"Apple\",\n",
    "    \"MSFT\": \"Microsoft\",\n",
    "    \"GOOG\": \"Alphabet\",\n",
    "    \"AMZN\": \"Amazon\",\n",
    "    \"TSLA\": \"Tesla\",\n",
    "    \"META\": \"Meta\",\n",
    "    \"NFLX\": \"Netflix\",\n",
    "    \"NVDA\": \"NVIDIA\",\n",
    "    \"AMD\": \"Advanced Micro Devices\",\n",
    "    \"BA\": \"Boeing\",\n",
    "    \"V\": \"Visa\",\n",
    "    \"SPY\": \"SPDR S&P 500 ETF\",\n",
    "    \"SPCE\": \"Virgin Galactic\",\n",
    "    \"FB\": \"Facebook\", \n",
    "    \"TWTR\": \"Twitter\", \n",
    "    \"BABA\": \"Alibaba\",\n",
    "    \"MSTR\": \"MicroStrategy\",\n",
    "    \"DIS\": \"Disney\",\n",
    "    \"PYPL\": \"PayPal\",\n",
    "    \"SHOP\": \"Shopify\",\n",
    "    \"COIN\": \"Coinbase\",\n",
    "    \"SQ\": \"Block\",\n",
    "    \"INTC\": \"Intel\",\n",
    "    \"CSCO\": \"Cisco\",\n",
    "    \"IBM\": \"IBM\",\n",
    "    \"GE\": \"General Electric\",\n",
    "    \"WMT\": \"Walmart\",\n",
    "    \"T\": \"AT&T\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transcript_links(ticker):\n",
    "    \"\"\"\n",
    "    Fetch up to three transcript links for the given ticker from Seeking Alpha.\n",
    "    \"\"\"\n",
    "    url = base_transcripts_url.format(ticker=ticker)\n",
    "    print(f\"\\nFetching transcript links for {ticker} from {url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page for {ticker} (Status code: {response.status_code})\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = [\n",
    "        \"https://seekingalpha.com\" + a[\"href\"]\n",
    "        for a in soup.find_all(\"a\", href=True)\n",
    "        if \"earnings-call-transcript\" in a[\"href\"]\n",
    "    ]\n",
    "    unique_links = list(dict.fromkeys(links))\n",
    "    return unique_links[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_transcript(transcript_url):\n",
    "    \"\"\"\n",
    "    Scrape the transcript text from the transcript URL.\n",
    "    \"\"\"\n",
    "    print(f\"\\nFetching transcript from: {transcript_url}\")\n",
    "    response = requests.get(transcript_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching transcript (Status code: {response.status_code})\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Extract all paragraph tags and join their text\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    transcript_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "    return transcript_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # List to store transcript data for the DataFrame\n",
    "    data = []\n",
    "\n",
    "    for ticker, company_name in ticker_company_map.items():\n",
    "        print(f\"\\n--- Processing {ticker} ({company_name}) ---\")\n",
    "        transcript_links = fetch_transcript_links(ticker)\n",
    "        \n",
    "        if not transcript_links:\n",
    "            print(f\"No transcripts found for {ticker}.\")\n",
    "        else:\n",
    "            for idx, transcript_url in enumerate(transcript_links, start=1):\n",
    "                transcript_text = scrape_transcript(transcript_url)\n",
    "                if transcript_text:\n",
    "                    # Store the transcript data as a dictionary\n",
    "                    data.append({\n",
    "                        \"ticker\": ticker,\n",
    "                        \"company\": company_name,\n",
    "                        \"transcript_number\": idx,\n",
    "                        \"transcript_url\": transcript_url,\n",
    "                        \"transcript_text\": transcript_text\n",
    "                    })\n",
    "                    # Optionally, print a preview of the transcript\n",
    "                    print(f\"\\nTranscript {idx} Preview for {ticker}:\\n{transcript_text[:500]}...\\n\")\n",
    "                else:\n",
    "                    print(f\"Transcript {idx} for {ticker} could not be fetched.\")\n",
    "                \n",
    "                print(\"Sleeping for 5 seconds before next transcript request...\\n\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        print(\"Sleeping for 10 seconds before processing the next ticker...\\n\")\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\n--- Final DataFrame ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Optionally, save the DataFrame to a CSV file\n",
    "    df.to_csv(\"earnings_transcripts.csv\", index=False)\n",
    "    print(\"\\nData saved to earnings_transcripts.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing AAPL (Apple) ---\n",
      "\n",
      "Fetching transcript links for AAPL from https://seekingalpha.com/symbol/AAPL/earnings/transcripts\n",
      "Error fetching page for AAPL (Status code: 403)\n",
      "No transcripts found for AAPL.\n",
      "Sleeping for 10 seconds before processing the next ticker...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping for 10 seconds before processing the next ticker...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the collected data\u001b[39;00m\n\u001b[1;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
