{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManagementDB():\n",
    "    def __init__(self, db_name='articlesWSJ.db'):\n",
    "        self.name = db_name\n",
    "        self.conn = sqlite3.connect(self.name)\n",
    "        self.c = self.conn.cursor()\n",
    "\n",
    "    def insert_elements(self, elements):\n",
    "        try:\n",
    "            self.c.execute(\"INSERT INTO articles_index (headline, article_time, year, month, day, keyword, link, scraped_at, scanned_status) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                  (elements[\"headline\"], elements[\"article_time\"], elements[\"year\"], elements[\"month\"], elements[\"day\"], elements[\"keyword\"],\n",
    "                   elements[\"link\"], elements[\"scraped_at\"], elements[\"scanned_status\"]))\n",
    "            self.conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def exploration(self, link, day, month, year, page_num, values_or_not, count_articles):\n",
    "        try:\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "            self.c.execute(''' INSERT INTO exploration (link, day, month, year, page_num, checked_at, values_or_not, count_articles)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (link, day, month, year, page_num, current_time, values_or_not, count_articles))\n",
    "    \n",
    "            self.conn.commit()\n",
    "        \n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    def closeDB(self):\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScrap:\n",
    "    def __init__(self):\n",
    "        self.page_number = 1\n",
    "        self.total_articles = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.page_number = 1\n",
    "        self.total_articles = 0\n",
    "        \n",
    "    def _save_to_json(self, article_details, year, month, day):\n",
    "        title_json = f\"article_titles_json/index_{year}_{month}_{day}_page_{self.page_number}.json\"\n",
    "        with open(title_json, 'w') as f:\n",
    "            json.dump(article_details, f)\n",
    "        print(f'Article details saved to {title_json}')\n",
    "\n",
    "    def get_elements_from_web(self, year, month, day, waiting_time):\n",
    "\n",
    "        db = ManagementDB()\n",
    "        end_page = False\n",
    "        \n",
    "        while not end_page:\n",
    "            \n",
    "            title_url = f'https://www.wsj.com/news/archive/{year}/{month}/{day}?page={self.page_number}'\n",
    "            print(title_url)\n",
    "    \n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT x.y; Win64; x64; rv:10.0) Gecko/20100101 Firefox/10.0 '}\n",
    "            page = requests.get(title_url, headers=headers)\n",
    "            \n",
    "            article_details = []\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if page.status_code == 200:\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                print('Page status code:', page.status_code)\n",
    "\n",
    "                # Find the <ol> element with the class 'WSJTheme--list-reset--3pR-r52l'\n",
    "                ol_element = soup.find('ol', class_='WSJTheme--list-reset--3pR-r52l')\n",
    "        \n",
    "                if ol_element:\n",
    "                    # Find all <article> elements within that <ol> element\n",
    "                    article_elements = ol_element.find_all('article')\n",
    "            \n",
    "                    if not article_elements:\n",
    "\n",
    "                        db.exploration(title_url, day, month, year, self.page_number, 0, 0)\n",
    "                        end_page = True\n",
    "                        self.reset()\n",
    "                        \n",
    "                    else:\n",
    "                        count_articles = 0\n",
    "                        # Extract required information from each <article> element\n",
    "                        for article in article_elements:\n",
    "                            headline_span = article.find('span', class_='WSJTheme--headlineText--He1ANr9C')\n",
    "                            a_tag = article.find('a')\n",
    "                            #article_type_span = article.find('span', class_='WSJTheme--articleType--34Gt-vdG')\n",
    "                            timestamp_p = article.find('p', class_='WSJTheme--timestamp--22sfkNDv')\n",
    "\n",
    "                            headline_text = headline_span.text if headline_span else \"N/A\"\n",
    "                            article_link = a_tag['href'] if a_tag else \"N/A\"\n",
    "                            #article_type = article_type_span.text if article_type_span else \"N/A\"\n",
    "                            article_time = timestamp_p.text if timestamp_p else \"N/A\"\n",
    "\n",
    "                            ####Article type####\n",
    "                            article_type_div = article.find('div', class_='WSJTheme--articleType--34Gt-vdG')\n",
    "                            empty_class_span = None\n",
    "\n",
    "                            if article_type_div:\n",
    "                                empty_class_span = article_type_div.find('span', class_='')\n",
    "\n",
    "                            article_type_text = empty_class_span.text if empty_class_span else \"N/A\"\n",
    "                            ####################\n",
    "\n",
    "                            # Adding the current local time of scraping\n",
    "                            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "                            dict_elements = {\n",
    "                                'headline': headline_text,\n",
    "                                'article_time': article_time,\n",
    "                \n",
    "                                'year' : year,\n",
    "                                'month' : month,\n",
    "                                'day' : day,\n",
    "                \n",
    "                                'keyword': article_type_text,\n",
    "                                'link': article_link,\n",
    "                \n",
    "                                'scraped_at': current_time,\n",
    "                                'scanned_status':0,\n",
    "                            }\n",
    "\n",
    "                            article_details.append(dict_elements)\n",
    "\n",
    "                            db.insert_elements(dict_elements)\n",
    "                    \n",
    "                            count_articles += 1\n",
    "                       \n",
    "                        \n",
    "                        self._save_to_json(article_details, year, month, day)\n",
    "\n",
    "                        db.exploration(title_url, day, month, year, self.page_number, 1, count_articles) #Page explored\n",
    "                        self.total_articles = self.total_articles + count_articles\n",
    "                        \n",
    "                        if count_articles == 50:\n",
    "                            self.page_number +=1\n",
    "                            time.sleep(waiting_time)\n",
    "                        else:\n",
    "                            print(f'Articles in the day {self.total_articles}')\n",
    "                            end_page = True\n",
    "                            #db.closeDB()\n",
    "                            self.reset()\n",
    "                        \n",
    "                else:\n",
    "                    print(\"Could not find <ol> element with the specified class.\")\n",
    "                    end_page = True\n",
    "                    self.reset()\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page. Status code: {page.status_code}\")\n",
    "                end_page = True\n",
    "                self.reset()\n",
    "        \n",
    "        db.closeDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(year):\n",
    "    start_date = datetime(year, 1, 1)\n",
    "    end_date = datetime(year, 12, 31)\n",
    "    date_list = []\n",
    "    \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append([current_date.day, current_date.month, current_date.year])\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching(year, waiting_time):\n",
    "    dates = get_dates(year)\n",
    "    scrap = WebScrap()\n",
    "    for day, month, year in dates:\n",
    "        print(day, month, year)\n",
    "        scrap.get_elements_from_web(year, month, day, waiting_time)\n",
    "        time.sleep(waiting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    year = 2015\n",
    "    waiting_time = 7\n",
    "    \n",
    "    \n",
    "    searching(year, waiting_time)\n",
    "    print(\"end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocksflags",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
